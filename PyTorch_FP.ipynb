{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNwMI7DZdxlMR1kJlpM1ORw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/CameronLarsonFLT/PyTorch_FP_Prediction/blob/main/PyTorch_FP.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://raw.githubusercontent.com/CameronLarsonFLT/PyTorch_FP_Prediction/main/FPs.png\" width=\"450\" align=\"right\">\n",
        "\n",
        "##PyTorch FP Property Predictor\n",
        "\n",
        "> **Example script** demonstrating how to:\n",
        "- Retrieve fluorescent protein (FP) data from the **FPbase REST API**\n",
        "- Train a simple **PyTorch neural network**\n",
        "- Predict key **spectral / photophysical properties** from an **amino-acid sequence**\n",
        "\n",
        "**Inputs:** `protein sequence (AA)`  \n",
        "**Outputs:** `ex_max`, `em_max`, `brightness`, `pKa`, `stokes_shift`"
      ],
      "metadata": {
        "id": "4vNJCkkITE2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "Example script demonstrating how to retrieve fluorescent protein data from the\n",
        "FPbase REST API and train a simple neural network using PyTorch to predict\n",
        "various spectral or photophysical properties from the amino-acid sequence.\n",
        "\"\"\"\n",
        "\n",
        "import requests\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange\n",
        "\n",
        "\n",
        "#@markdown ---\n",
        "#@markdown ## Property selection\n",
        "#@markdown **Accepted values for `PROPERTY_NAME`:**\n",
        "#@markdown\n",
        "#@markdown - **Spectral**\n",
        "#@markdown   - `ex_max` — excitation maximum *(nm)*\n",
        "#@markdown   - `em_max` — emission maximum *(nm)*\n",
        "#@markdown - **Photophysics**\n",
        "#@markdown   - `brightness` — FPbase brightness\n",
        "#@markdown   - `pka` — chromophore pKa\n",
        "#@markdown ---\n",
        "\n",
        "PROPERTY_NAME = 'brightness' #@param {type:\"string\"}\n",
        "\n",
        "#Define a Fluorescent Protein Sequence for Train-Test Validation\n",
        "Fluorescent_Protein_Seq = 'MSKGEELFTGVVPILVELDGDVNGHKFSVSGEGEGDATYGKLTLKFICTTGKLPVPWPTLVTTFSYGVQCFSRYPDHMKQHDFFKSAMPEGYVQERTIFFKDDGNYKTRAEVKFEGDTLVNRIELKGIDFKEDGNILGHKLEYNYNSHNVYIMADKQKNGIKVNFKIRHNIEDGSVQLADHYQQNTPIGDGPVLLPDNHYLSTQSALSKDPNEKRDHMVLLEFVTAAGITHGMDELYK' #@param {type:\"string\"}\n",
        "\n",
        "#@markdown - **Commonly Used Taxonomy IDs**\n",
        "#@markdown   - `6100`  *(Aequoria victoria)*\n",
        "#@markdown   - `86600` *(Discosoma sp)*\n",
        "#@markdown   - `6118`  *(Entacmaea quadricolor)*\n",
        "#@markdown - **Leave Blank to Train on Full FPbase.org Sequence Data**\n",
        "PARENT_ORGANISM = 6100  #@param {type:\"integer\"}\n",
        "\n",
        "#@markdown **Training epochs** *(recommended: 5000)*\n",
        "\n",
        "EPOCHS = 5000 #@param {type:\"integer\"}\n",
        "\n",
        "EXCLUDE_TERMS = (\"channelrhodopsin\", \"rcamp\", \"gcamp2\", \"cp-mkate\", \"cegfp\")\n",
        "AMINO_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "def fetch_fpbase_proteins(parent_organism) -> list:\n",
        "    \"\"\"Retrieve proteins for a given parent organism from the FPbase API.\"\"\"\n",
        "    proteins: list = []\n",
        "\n",
        "    # Normalize the FUNCTION ARGUMENT (not the global), so int/string/blank all work\n",
        "    parent_organism = \"\" if parent_organism is None else str(parent_organism).strip()\n",
        "\n",
        "    # Allow user to \"leave blank\" (and tolerate common null-ish strings)\n",
        "    if parent_organism.lower() in (\"\", \"none\", \"null\"):\n",
        "        parent_organism = \"\"\n",
        "\n",
        "    if parent_organism:\n",
        "        if not parent_organism.isdigit():\n",
        "            raise ValueError(\n",
        "                f\"PARENT_ORGANISM must be numeric or blank. Got: {parent_organism!r}\"\n",
        "            )\n",
        "        url = f\"https://www.fpbase.org/api/proteins/?parent_organism={parent_organism}&format=json\"\n",
        "    else:\n",
        "        print(\"TRAINING ON FULL FPBASE.ORG SEQUENCE DATA\")\n",
        "        url = \"https://www.fpbase.org/api/proteins/?&format=json\"\n",
        "\n",
        "    while url:\n",
        "        print(f\"Fetching {url} …\")\n",
        "        resp = requests.get(url)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if isinstance(data, list):\n",
        "            proteins.extend(data)\n",
        "            break\n",
        "        results = data.get(\"results\", [])\n",
        "        proteins.extend(results)\n",
        "        url = data.get(\"next\")\n",
        "    return proteins\n",
        "\n",
        "\n",
        "def compute_stokes_shift(protein: dict) -> float | None:\n",
        "    for state in protein.get(\"states\", []):\n",
        "        ex = state.get(\"ex_max\")\n",
        "        em = state.get(\"em_max\")\n",
        "        if ex is not None and em is not None:\n",
        "            return float(em) - float(ex)\n",
        "    return None\n",
        "\n",
        "\n",
        "def extract_property(protein: dict, property_name: str) -> float | None:\n",
        "    if property_name == 'stokes_shift':\n",
        "        return compute_stokes_shift(protein)\n",
        "    states = protein.get(\"states\", [])\n",
        "    if states and isinstance(states, list):\n",
        "        return states[0].get(property_name)\n",
        "    return None\n",
        "\n",
        "\n",
        "def one_hot_encode_sequence(seq: str, max_len: int, alphabet: str = AMINO_ALPHABET) -> np.ndarray:\n",
        "    aa_to_idx = {aa: i for i, aa in enumerate(alphabet)}\n",
        "    encoding = np.zeros((max_len, len(alphabet)), dtype=np.float32)\n",
        "    for i, aa in enumerate(seq[:max_len]):\n",
        "        idx = aa_to_idx.get(aa)\n",
        "        if idx is not None:\n",
        "            encoding[i, idx] = 1.0\n",
        "    return encoding\n",
        "\n",
        "\n",
        "class PropertyPredictor(nn.Module):\n",
        "    def __init__(self, input_len: int, alphabet_size: int, hidden_dim: int = 64):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_len * alphabet_size, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        return self.net(x)\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "def main() -> None:\n",
        "    import sys\n",
        "    global PROPERTY_NAME\n",
        "\n",
        "    allowed_properties = {\"stokes_shift\", \"ex_max\", \"em_max\", \"brightness\", \"pka\"}\n",
        "    for arg in sys.argv[1:]:\n",
        "        if arg in allowed_properties:\n",
        "            PROPERTY_NAME = arg\n",
        "            break\n",
        "    print(f\"Predicting property: {PROPERTY_NAME}\")\n",
        "\n",
        "    proteins = fetch_fpbase_proteins(parent_organism=PARENT_ORGANISM)\n",
        "\n",
        "    proteins = [\n",
        "        p for p in proteins\n",
        "        if not any(term in (p.get(\"name\") or \"\").lower() for term in EXCLUDE_TERMS)\n",
        "    ]\n",
        "\n",
        "    samples: list[tuple[str, float]] = []\n",
        "    for protein in proteins:\n",
        "        seq = protein.get(\"seq\")\n",
        "        value = extract_property(protein, PROPERTY_NAME)\n",
        "        if seq and value is not None:\n",
        "            try:\n",
        "                samples.append((seq, float(value)))\n",
        "            except Exception:\n",
        "                pass\n",
        "\n",
        "    if not samples:\n",
        "        raise RuntimeError(f\"No valid samples retrieved for property '{PROPERTY_NAME}'.\")\n",
        "\n",
        "    max_len = max(len(seq) for seq, _ in samples)\n",
        "\n",
        "    X = np.stack([one_hot_encode_sequence(seq, max_len, AMINO_ALPHABET) for seq, _ in samples])\n",
        "    y = np.array([val for _, val in samples], dtype=np.float32)\n",
        "\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1)\n",
        "\n",
        "    model = PropertyPredictor(input_len=max_len, alphabet_size=len(AMINO_ALPHABET))\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    epochs = EPOCHS\n",
        "    print(\"Training neural network...\")\n",
        "    pbar = trange(\n",
        "        epochs,\n",
        "        desc=\"Training\",\n",
        "        unit=\"epoch\",\n",
        "        leave=True,\n",
        "        colour='cyan',\n",
        "        bar_format=\"{desc} {n_fmt}/{total_fmt} | {postfix}\",\n",
        "    )\n",
        "\n",
        "    loss_history = []\n",
        "    for _ in pbar:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_tensor)\n",
        "        loss = criterion(preds, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "        pbar.set_postfix_str(f\"loss={loss.item():.3f}\", refresh=True)\n",
        "\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_tensor).squeeze().cpu().numpy()\n",
        "\n",
        "    # ---- EGFP prediction + annotation ----\n",
        "    egfp_enc = one_hot_encode_sequence(Fluorescent_Protein_Seq, max_len, AMINO_ALPHABET)\n",
        "    egfp_tensor = torch.tensor(egfp_enc, dtype=torch.float32).unsqueeze(0)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        egfp_pred = float(model(egfp_tensor).squeeze().item())\n",
        "\n",
        "    print(f\"EGFP predicted {PROPERTY_NAME}: {egfp_pred:.3f}\")\n",
        "\n",
        "    plt.figure()\n",
        "    plt.plot(range(1, epochs + 1), loss_history)\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Training Loss (MSE)')\n",
        "    plt.title(f'Training Loss for predicting {PROPERTY_NAME}')\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'training_loss_{PROPERTY_NAME}.png')\n",
        "    plt.show()\n",
        "\n",
        "    plt.figure()\n",
        "    plt.scatter(y, predictions, alpha=0.4)\n",
        "    min_val = float(min(y.min(), predictions.min(), egfp_pred))\n",
        "    max_val = float(max(y.max(), predictions.max(), egfp_pred))\n",
        "    plt.plot([min_val, max_val], [min_val, max_val], linestyle='--')\n",
        "\n",
        "    # Place EGFP on the diagonal for visibility (EGFP has no \"observed\" label here)\n",
        "    plt.scatter([egfp_pred], [egfp_pred], s=200, marker='o', edgecolors='k',\n",
        "                label='FP_Seq', color='yellow')\n",
        "    plt.annotate(\n",
        "        f\"FP_Seq\\npred={egfp_pred:.2f}\",\n",
        "        xy=(egfp_pred, egfp_pred),\n",
        "        xytext=(10, -25),\n",
        "        textcoords=\"offset points\",\n",
        "        arrowprops=dict(arrowstyle=\"->\")\n",
        "    )\n",
        "\n",
        "    plt.xlabel(f'Observed {PROPERTY_NAME}')\n",
        "    plt.ylabel(f'Predicted {PROPERTY_NAME}')\n",
        "    plt.title(f'Predicted vs Observed {PROPERTY_NAME} (Sequence annotated)')\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(f'predicted_vs_observed_{PROPERTY_NAME}.png')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ],
      "metadata": {
        "id": "PZJ7p0-_JoBE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "8111a074-c7b3-40e8-8381-40cd48b6a196",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-2839298369.py, line 3)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-2839298369.py\"\u001b[0;36m, line \u001b[0;32m3\u001b[0m\n\u001b[0;31m    <img src=\"https://raw.githubusercontent.com/CameronLarsonFLT/PyTorch_FP_Prediction/main/FPs.png\" width=\"450\" align=\"right\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wQWJxzm_4nGf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW7eDC_F6pa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GsYRPXn46pdt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qiO6t6I_6pk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GG5R4VrN6ppJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D8IvtDK56prw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tklrWJpdYaxr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aAJYATJD6puW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yWM1vDIrmMzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gZU4EV3CmM3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "px4M4iXzmM5u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "WOfXvEzj6pws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qcJkffHL4nKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "PARENT_ORGANISM = 6100\n",
        "\n",
        "# Exclude protein names containing any of these substrings\n",
        "EXCLUDE_TERMS = (\"channelrhodopsin\", \"rcamp\", \"gcamp2\", \"cp-mkate\", \"cegfp\")\n",
        "\n",
        "# Amino acid alphabet for one‑hot encoding\n",
        "AMINO_ALPHABET = \"ACDEFGHIKLMNPQRSTVWY\"\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "def fetch_fpbase_proteins(parent_organism: int) -> list:\n",
        "    \"\"\"Retrieve proteins for a given parent organism from the FPbase API.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    parent_organism : int\n",
        "        Numeric identifier for the organism.  For example, 6100 corresponds\n",
        "        to fluorescent proteins derived from Discosoma sp.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    list of dict\n",
        "        A list of protein objects as returned by the API.\n",
        "    \"\"\"\n",
        "    proteins: list = []\n",
        "    # url = f\"https://www.fpbase.org/api/proteins/?parent_organism={parent_organism}&format=json\"\n",
        "    url = f\"https://www.fpbase.org/api/proteins/?&format=json\"\n",
        "\n",
        "    while url:\n",
        "        print(f\"Fetching {url} …\")\n",
        "        resp = requests.get(url)\n",
        "        resp.raise_for_status()\n",
        "        data = resp.json()\n",
        "        if isinstance(data, list):\n",
        "            proteins.extend(data)\n",
        "            break\n",
        "        results = data.get(\"results\", [])\n",
        "        proteins.extend(results)\n",
        "        url = data.get(\"next\")\n",
        "    return proteins"
      ],
      "metadata": {
        "id": "hHI9NK0y4nNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wraeDGeOmOP7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_trained_bundle(\n",
        "    path: str,\n",
        "    model,\n",
        "    *,\n",
        "    max_len: int,\n",
        "    amino_alphabet: str,\n",
        "    property_name: str\n",
        "):\n",
        "    \"\"\"\n",
        "    Save everything needed to reload the model and run inference later without retraining.\n",
        "    \"\"\"\n",
        "    bundle = {\n",
        "        \"state_dict\": model.state_dict(),\n",
        "        \"max_len\": int(max_len),\n",
        "        \"amino_alphabet\": amino_alphabet,\n",
        "        \"property_name\": property_name,\n",
        "        \"model_kwargs\": {\n",
        "            \"input_len\": int(max_len),\n",
        "            \"alphabet_size\": int(len(amino_alphabet)),\n",
        "        },\n",
        "    }\n",
        "    Path(path).parent.mkdir(parents=True, exist_ok=True)\n",
        "    torch.save(bundle, path)\n",
        "\n",
        "def load_trained_bundle(path: str, model_cls, device: str = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Load a saved model and return (model, bundle dict).\n",
        "    \"\"\"\n",
        "    bundle = torch.load(path, map_location=device)\n",
        "    model = model_cls(**bundle[\"model_kwargs\"])\n",
        "    model.load_state_dict(bundle[\"state_dict\"])\n",
        "    model.eval()\n",
        "    return model.to(device), bundle"
      ],
      "metadata": {
        "id": "xWovoC5-lKwp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "85uRGh7alK3t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QHdBLD1nlK6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Q-K36_8lK83"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This script trains fluorescent protein property prediction models for one or more properties.\n",
        "\n",
        "It mirrors the existing `main()` training loop that trains a single-property model but\n",
        "allows the caller to specify multiple properties to train sequentially. For each property,\n",
        "the script will:\n",
        "\n",
        "  * Fetch and filter FPbase sequences using `fetch_fpbase_proteins` and `EXCLUDE_TERMS`.\n",
        "  * Build one-hot encoded input tensors based on `AMINO_ALPHABET`.\n",
        "  * Train an instance of `PropertyPredictor` on the selected property using mean squared\n",
        "    error as the loss function.\n",
        "  * Save the trained model bundle to disk via `save_trained_bundle`.\n",
        "  * Optionally plot the training loss history.\n",
        "\n",
        "Assumptions:\n",
        "  - Functions `fetch_fpbase_proteins`, `extract_property`, `one_hot_encode_sequence`,\n",
        "    the `PropertyPredictor` class, and constants `PARENT_ORGANISM`, `EXCLUDE_TERMS`,\n",
        "    `AMINO_ALPHABET` exist in the environment that imports this module.\n",
        "  - Functions `save_trained_bundle` and `load_trained_bundle` are also available in\n",
        "    the environment. These are used to save and reload trained model bundles.\n",
        "\n",
        "Example usage:\n",
        "\n",
        "    from train_fp_properties import train_fp_properties\n",
        "\n",
        "    # train brightness and pKa models, forcing retrain and using GPU if available\n",
        "    results = train_fp_properties(\n",
        "        property_names=[\"brightness\", \"pka\"],\n",
        "        input_seq=my_sequence,\n",
        "        epochs=3000,\n",
        "        force_retrain=True,\n",
        "        model_dir=\"models\",\n",
        "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
        "        make_plots=True,\n",
        "    )\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "from typing import Iterable, Dict, Tuple, List\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import trange\n",
        "\n",
        "# Ensure required helpers and constants exist in the importing namespace. If any of\n",
        "# these are missing, a NameError will be raised when this module is imported. The\n",
        "# caller should define or import these before importing this module.\n",
        "try:\n",
        "    fetch_fpbase_proteins  # type: ignore[name-defined]\n",
        "    extract_property  # type: ignore[name-defined]\n",
        "    one_hot_encode_sequence  # type: ignore[name-defined]\n",
        "    PropertyPredictor  # type: ignore[name-defined]\n",
        "    PARENT_ORGANISM  # type: ignore[name-defined]\n",
        "    EXCLUDE_TERMS  # type: ignore[name-defined]\n",
        "    AMINO_ALPHABET  # type: ignore[name-defined]\n",
        "    save_trained_bundle  # type: ignore[name-defined]\n",
        "    load_trained_bundle  # type: ignore[name-defined]\n",
        "except NameError as e:\n",
        "    raise ImportError(\n",
        "        \"Required helper functions or constants are not defined in the importing namespace: \"\n",
        "        f\"{e}\"\n",
        "    )\n",
        "\n",
        "from pathlib import Path\n",
        "import torch\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def _predict_sequence_local(\n",
        "    model: torch.nn.Module,\n",
        "    seq: str,\n",
        "    max_len: int,\n",
        "    alphabet: str,\n",
        "    device: str,\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Local helper to predict a scalar property for a single sequence using the trained model.\n",
        "    This function does not rely on any external helper and uses the local encoding.\n",
        "\n",
        "    Args:\n",
        "        model: Trained PyTorch model that accepts (batch, L, A) inputs and outputs (batch, 1).\n",
        "        seq: Amino acid sequence to encode.\n",
        "        max_len: Maximum length used for training (for padding/truncation).\n",
        "        alphabet: String representing the alphabet order used during training.\n",
        "        device: Device string (e.g. \"cpu\" or \"cuda\") where the model resides.\n",
        "\n",
        "    Returns:\n",
        "        A single float prediction for the provided sequence.\n",
        "    \"\"\"\n",
        "    enc = one_hot_encode_sequence(seq, max_len, alphabet)  # type: ignore[name-defined]\n",
        "    x = torch.tensor(enc, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        return float(model(x).squeeze().item())\n",
        "\n",
        "\n",
        "def _train_single_property(\n",
        "    property_name: str,\n",
        "    *,\n",
        "    input_seq: str,\n",
        "    epochs: int,\n",
        "    force_retrain: bool,\n",
        "    model_dir: str,\n",
        "    device: str,\n",
        "    make_plots: bool,\n",
        ") -> Tuple[\n",
        "    torch.nn.Module,\n",
        "    int,\n",
        "    torch.Tensor,\n",
        "    torch.Tensor,\n",
        "    np.ndarray,\n",
        "    float,\n",
        "]:\n",
        "    \"\"\"\n",
        "    Train or load a model for a single FP property.\n",
        "\n",
        "    Args:\n",
        "        property_name: Property to train on (e.g. \"brightness\", \"pka\").\n",
        "        input_seq: Sequence used for immediate prediction after training.\n",
        "        epochs: Number of training epochs.\n",
        "        force_retrain: If True, ignore any saved model and retrain from scratch.\n",
        "        model_dir: Directory where the model bundle is saved/loaded.\n",
        "        device: Target device for training (\"cpu\" or \"cuda\").\n",
        "        make_plots: If True, plot the training loss history.\n",
        "\n",
        "    Returns:\n",
        "        A tuple containing:\n",
        "          - The trained model\n",
        "          - The maximum sequence length used for encoding\n",
        "          - The tensor of one-hot encoded training sequences\n",
        "          - The tensor of training labels\n",
        "          - A numpy array of model predictions on the training set\n",
        "          - The model's prediction for the provided input_seq\n",
        "    \"\"\"\n",
        "    # Ensure model directory exists\n",
        "    os.makedirs(model_dir, exist_ok=True)\n",
        "    model_path = os.path.join(model_dir, f\"fp_{property_name}.pt\")\n",
        "\n",
        "    # If a saved model exists and retrain is not forced, load and return predictions\n",
        "    if not force_retrain and os.path.exists(model_path):\n",
        "        model, bundle = load_trained_bundle(model_path, PropertyPredictor, device=device)  # type: ignore[name-defined]\n",
        "        max_len = int(bundle[\"max_len\"])\n",
        "        amino_alphabet = bundle[\"amino_alphabet\"]\n",
        "\n",
        "        input_pred = _predict_sequence_local(model, input_seq, max_len, amino_alphabet, device)\n",
        "\n",
        "        # Rebuild training data for evaluation and plotting\n",
        "        proteins = fetch_fpbase_proteins(parent_organism=PARENT_ORGANISM)  # type: ignore[name-defined]\n",
        "        proteins = [\n",
        "            p\n",
        "            for p in proteins\n",
        "            if not any(term in (p.get(\"name\") or \"\").lower() for term in EXCLUDE_TERMS)  # type: ignore[name-defined]\n",
        "        ]\n",
        "        samples: List[Tuple[str, float]] = []\n",
        "        for protein in proteins:\n",
        "            seq = protein.get(\"seq\")\n",
        "            val = extract_property(protein, property_name)  # type: ignore[name-defined]\n",
        "            if seq and val is not None:\n",
        "                try:\n",
        "                    samples.append((seq, float(val)))\n",
        "                except Exception:\n",
        "                    pass\n",
        "        if not samples:\n",
        "            raise RuntimeError(\n",
        "                f\"No valid samples retrieved for property '{property_name}' when loading model.\"\n",
        "            )\n",
        "        X = np.stack([\n",
        "            one_hot_encode_sequence(seq, max_len, amino_alphabet)  # type: ignore[name-defined]\n",
        "            for seq, _ in samples\n",
        "        ])\n",
        "        y = np.array([val for _, val in samples], dtype=np.float32)\n",
        "        X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "        y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = model(X_tensor).squeeze().detach().cpu().numpy()\n",
        "        return model, max_len, X_tensor, y_tensor, preds, input_pred\n",
        "\n",
        "    # Otherwise train from scratch\n",
        "    proteins = fetch_fpbase_proteins(parent_organism=PARENT_ORGANISM)  # type: ignore[name-defined]\n",
        "    proteins = [\n",
        "        p\n",
        "        for p in proteins\n",
        "        if not any(term in (p.get(\"name\") or \"\").lower() for term in EXCLUDE_TERMS)  # type: ignore[name-defined]\n",
        "    ]\n",
        "    samples: List[Tuple[str, float]] = []\n",
        "    for protein in proteins:\n",
        "        seq = protein.get(\"seq\")\n",
        "        val = extract_property(protein, property_name)  # type: ignore[name-defined]\n",
        "        if seq and val is not None:\n",
        "            try:\n",
        "                samples.append((seq, float(val)))\n",
        "            except Exception:\n",
        "                pass\n",
        "    if not samples:\n",
        "        raise RuntimeError(f\"No valid samples retrieved for property '{property_name}'.\")\n",
        "\n",
        "    max_len = max(len(seq) for seq, _ in samples)\n",
        "    X = np.stack(\n",
        "        [one_hot_encode_sequence(seq, max_len, AMINO_ALPHABET)  # type: ignore[name-defined]\n",
        "         for seq, _ in samples]\n",
        "    )\n",
        "    y = np.array([val for _, val in samples], dtype=np.float32)\n",
        "    X_tensor = torch.tensor(X, dtype=torch.float32).to(device)\n",
        "    y_tensor = torch.tensor(y, dtype=torch.float32).unsqueeze(1).to(device)\n",
        "\n",
        "    model = PropertyPredictor(\n",
        "        input_len=max_len,\n",
        "        alphabet_size=len(AMINO_ALPHABET),\n",
        "    ).to(device)\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
        "\n",
        "    pbar = trange(\n",
        "        epochs,\n",
        "        desc=f\"Training {property_name}\",\n",
        "        unit=\"epoch\",\n",
        "        leave=True,\n",
        "        colour=\"cyan\",\n",
        "        bar_format=\"{l_bar}{bar}| {n_fmt}/{total_fmt} [ETA {remaining}] {postfix}\",\n",
        "    )\n",
        "    loss_history: List[float] = []\n",
        "    for _ in pbar:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        preds = model(X_tensor)\n",
        "        loss = criterion(preds, y_tensor)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_history.append(loss.item())\n",
        "        pbar.set_postfix_str(f\"loss={loss.item():.3f}\", refresh=True)\n",
        "\n",
        "    # Save the trained model bundle for future use\n",
        "    save_trained_bundle(\n",
        "        model_path,\n",
        "        model,\n",
        "        max_len=max_len,\n",
        "        amino_alphabet=AMINO_ALPHABET,  # type: ignore[name-defined]\n",
        "        property_name=property_name,\n",
        "    )\n",
        "\n",
        "    # Evaluate on the full training set for later analysis\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_tensor).squeeze().detach().cpu().numpy()\n",
        "\n",
        "    # Predict the property for the provided sequence\n",
        "    input_pred = _predict_sequence_local(\n",
        "        model,\n",
        "        input_seq,\n",
        "        max_len,\n",
        "        AMINO_ALPHABET,  # type: ignore[name-defined]\n",
        "        device,\n",
        "    )\n",
        "\n",
        "    # Optionally plot training loss\n",
        "    if make_plots:\n",
        "        plt.figure(figsize=(7, 4))\n",
        "        plt.plot(range(1, epochs + 1), loss_history)\n",
        "        plt.xlabel(\"Epoch\")\n",
        "        plt.ylabel(\"Training Loss (MSE)\")\n",
        "        plt.title(f\"Training Loss for {property_name}\")\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    return model, max_len, X_tensor, y_tensor, predictions, input_pred\n",
        "\n",
        "\n",
        "def train_fp_properties(\n",
        "    property_names: Iterable[str],\n",
        "    *,\n",
        "    input_seq: str,\n",
        "    epochs: int = 5000,\n",
        "    force_retrain: bool = False,\n",
        "    model_dir: str = \"models\",\n",
        "    device: str = \"cpu\",\n",
        "    make_plots: bool = False,\n",
        ") -> Dict[str, Tuple[torch.nn.Module, int, torch.Tensor, torch.Tensor, np.ndarray, float]]:\n",
        "    \"\"\"\n",
        "    Train (or load) models for a list of fluorescent protein properties.\n",
        "\n",
        "    This function is a convenience wrapper that iterates over `property_names`, calling\n",
        "    the single-property training routine for each. It returns a dictionary of results.\n",
        "\n",
        "    Args:\n",
        "        property_names: Iterable of property names to train (e.g., [\"brightness\", \"pka\"]).\n",
        "        input_seq: Amino acid sequence to evaluate immediately after training each model.\n",
        "        epochs: Number of epochs to train each model. Default is 5000.\n",
        "        force_retrain: If True, ignore existing saved models and retrain from scratch for\n",
        "            each property.\n",
        "        model_dir: Directory where model bundles will be saved and loaded from.\n",
        "        device: Device string (\"cpu\" or \"cuda\"). If \"cuda\" is specified but not\n",
        "            available, training will fall back to CPU silently.\n",
        "        make_plots: Whether to produce a training loss plot for each property.\n",
        "\n",
        "    Returns:\n",
        "        A dictionary keyed by property name. Each value is the tuple returned by\n",
        "        `_train_single_property` containing: (model, max_len, X_tensor, y_tensor,\n",
        "        predictions_on_training_set, input_prediction).\n",
        "\n",
        "    Notes:\n",
        "        If you specify a large list of properties and large numbers of epochs, the\n",
        "        training may take significant time. Consider lowering `epochs` or disabling\n",
        "        plots for quicker experimentation.\n",
        "    \"\"\"\n",
        "    results: Dict[str, Tuple[\n",
        "        torch.nn.Module,\n",
        "        int,\n",
        "        torch.Tensor,\n",
        "        torch.Tensor,\n",
        "        np.ndarray,\n",
        "        float,\n",
        "    ]] = {}\n",
        "    # Normalize device selection: if CUDA is requested but unavailable, use CPU\n",
        "    device = device if (device == \"cuda\" and torch.cuda.is_available()) else \"cpu\"\n",
        "    for prop in property_names:\n",
        "        print(f\"=== Training model for property: {prop} ===\")\n",
        "        results[prop] = _train_single_property(\n",
        "            prop,\n",
        "            input_seq=input_seq,\n",
        "            epochs=epochs,\n",
        "            force_retrain=force_retrain,\n",
        "            model_dir=model_dir,\n",
        "            device=device,\n",
        "            make_plots=make_plots,\n",
        "        )\n",
        "        print(f\"=== Completed training for property: {prop}\\n\")\n",
        "    return results"
      ],
      "metadata": {
        "id": "uQmvOgH7mOZz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "BSZTZzJ5Bvtq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "property_names = [\"brightness\", \"ex_max\", \"em_max\", 'pka']  # <-- set here\n",
        "results = train_fp_properties(\n",
        "    property_names=property_names,\n",
        "    input_seq=Fluorescent_Protein_Seq,\n",
        "    force_retrain=True,\n",
        "    model_dir=\"/content/drive/MyDrive/PyTorch/PyTorch_Training_Data\",\n",
        "    epochs=20000,\n",
        "    device=\"cuda\",\n",
        "    make_plots=False\n",
        ")"
      ],
      "metadata": {
        "id": "1P2F0CXcmOdP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ------------- CONFIGURATION -------------\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/PyTorch/PyTorch_Output_Data'\n",
        "FASTA_PATH = \"/content/drive/MyDrive/Machine_Learning_FP_Evolution/EGFP_Ex_Max_Reverse_Evolved_Sequences_From_ML.fasta\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/PyTorch/PyTorch_Training_Data\"\n",
        "PROPERTY_NAMES = [\"brightness\", \"pka\", \"ex_max\", \"em_max\",]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "# ---------- REQUIRED DEFINITIONS ----------\n",
        "# Ensure these exist in your notebook runtime:\n",
        "# - PropertyPredictor class (same architecture used during training)\n",
        "# - one_hot_encode_sequence(seq, max_len, alphabet)\n",
        "# If they are not in scope, import them or paste their definitions here.\n",
        "\n",
        "def load_trained_bundle(path: str, model_cls, device: str = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Load a saved model and return (model, bundle dict).\n",
        "    \"\"\"\n",
        "    bundle = torch.load(path, map_location=device)\n",
        "    model = model_cls(**bundle[\"model_kwargs\"])\n",
        "    model.load_state_dict(bundle[\"state_dict\"])\n",
        "    model.eval()\n",
        "    return model.to(device), bundle\n",
        "\n",
        "def read_fasta(path: str):\n",
        "    \"\"\"\n",
        "    Returns a list of dicts: {\"id\":..., \"desc\":..., \"seq\":...} for each record.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    with open(path, \"r\") as f:\n",
        "        seq_id, desc, buf = None, \"\", []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if line.startswith(\">\"):\n",
        "                if seq_id is not None:\n",
        "                    records.append({\"id\": seq_id, \"desc\": desc, \"seq\": \"\".join(buf)})\n",
        "                header = line[1:]\n",
        "                parts = header.split(None, 1)\n",
        "                seq_id = parts[0]\n",
        "                desc = parts[1] if len(parts) > 1 else \"\"\n",
        "                buf = []\n",
        "            else:\n",
        "                buf.append(line)\n",
        "        if seq_id is not None:\n",
        "            records.append({\"id\": seq_id, \"desc\": desc, \"seq\": \"\".join(buf)})\n",
        "    return records\n",
        "\n",
        "def sanitize_seq(seq: str):\n",
        "    \"\"\"Remove whitespace and uppercase a sequence.\"\"\"\n",
        "    return re.sub(r\"\\s+\", \"\", (seq or \"\").upper())\n",
        "\n",
        "def predict_property(model, seq: str, *, max_len: int, alphabet: str):\n",
        "    \"\"\"\n",
        "    Encode a sequence and produce a scalar prediction.\n",
        "    \"\"\"\n",
        "    enc = one_hot_encode_sequence(seq, max_len, alphabet)  # (max_len, alphabet_size)\n",
        "    x = torch.tensor(enc, dtype=torch.float32).unsqueeze(0).to(next(model.parameters()).device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        return float(model(x).squeeze().item())\n",
        "\n",
        "# ---------- LOAD ALL MODELS ----------\n",
        "# Each property gets its own model and encoding parameters.\n",
        "models_info = {}\n",
        "for prop in PROPERTY_NAMES:\n",
        "    path = os.path.join(MODEL_DIR, f\"fp_{prop}.pt\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Model file not found for {prop}: {path}\")\n",
        "    model, bundle = load_trained_bundle(path, PropertyPredictor, device=DEVICE)\n",
        "    models_info[prop] = {\n",
        "        \"model\": model,\n",
        "        \"max_len\": int(bundle[\"max_len\"]),\n",
        "        \"alphabet\": bundle[\"amino_alphabet\"],\n",
        "    }\n",
        "    print(f\"Loaded {prop} model with max_len={models_info[prop]['max_len']}\")\n",
        "\n",
        "# ---------- READ FASTA ----------\n",
        "records = read_fasta(FASTA_PATH)\n",
        "if not records:\n",
        "    raise RuntimeError(f\"No FASTA records found in {FASTA_PATH}\")\n",
        "print(\"Number of sequences:\", len(records))\n",
        "\n",
        "# ---------- PREDICT FOR ALL PROPERTIES ----------\n",
        "rows = []\n",
        "for rec in records:\n",
        "    seq_raw = sanitize_seq(rec[\"seq\"])\n",
        "    row = {\n",
        "        \"id\": rec[\"id\"],\n",
        "        \"desc\": rec[\"desc\"],\n",
        "        \"seq_len\": len(seq_raw),\n",
        "        \"sequence\": seq_raw,\n",
        "    }\n",
        "    # Track unknown/truncated characters per model (optional)\n",
        "    for prop in PROPERTY_NAMES:\n",
        "        info = models_info[prop]\n",
        "        alphabet_set = set(info[\"alphabet\"])\n",
        "        max_len = info[\"max_len\"]\n",
        "        row[f\"unknown_{prop}\"] = sum(1 for ch in seq_raw if ch not in alphabet_set)\n",
        "        row[f\"trunc_{prop}\"] = max(0, len(seq_raw) - max_len)\n",
        "        pred = predict_property(\n",
        "            info[\"model\"],\n",
        "            seq_raw,\n",
        "            max_len=max_len,\n",
        "            alphabet=info[\"alphabet\"],\n",
        "        )\n",
        "        row[f\"pred_{prop}\"] = pred\n",
        "    rows.append(row)\n",
        "\n",
        "# ---------- BUILD DATAFRAME ----------\n",
        "df = pd.DataFrame(rows)\n",
        "\n",
        "# Example: sort by brightness descending\n",
        "df_sorted = df.sort_values(by=\"pred_brightness\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "# print(df_sorted.head(10)[[\"id\", \"seq_len\"] + [f\"pred_{p}\" for p in PROPERTY_NAMES]])\n",
        "# Save to CSV if desired\n",
        "fasta_base = os.path.splitext(os.path.basename(FASTA_PATH))[0]\n",
        "\n",
        "out_csv = os.path.join(OUTPUT_PATH,fasta_base + \"_ANN_predictions_multi_parameters.csv\")\n",
        "df_sorted.to_csv(out_csv, index=False)\n",
        "print(\"Saved predictions:\", out_csv)\n"
      ],
      "metadata": {
        "id": "2USQ6ScMmOfz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nhG9XSGAmOic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import re\n",
        "import pandas as pd\n",
        "import torch\n",
        "import numpy as np\n",
        "\n",
        "# ------------- CONFIGURATION -------------\n",
        "OUTPUT_PATH = '/content/drive/MyDrive/PyTorch/PyTorch_Output_Data'\n",
        "FASTA_DIR = \"/content/drive/MyDrive/Machine_Learning_FP_Evolution\"\n",
        "MODEL_DIR = \"/content/drive/MyDrive/PyTorch/PyTorch_Training_Data\"\n",
        "PROPERTY_NAMES = [\"brightness\", \"pka\", \"ex_max\", \"em_max\",]\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using device:\", DEVICE)\n",
        "\n",
        "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
        "\n",
        "# Collect all fasta-like files in the directory\n",
        "fasta_files = [\n",
        "    os.path.join(FASTA_DIR, f)\n",
        "    for f in os.listdir(FASTA_DIR)\n",
        "    if f.lower().endswith((\".fasta\", \".fa\", \".faa\"))\n",
        "]\n",
        "print(\"Found FASTA files:\", len(fasta_files))\n",
        "\n",
        "# ---------- REQUIRED DEFINITIONS ----------\n",
        "# Ensure these exist in your notebook runtime:\n",
        "# - PropertyPredictor class (same architecture used during training)\n",
        "# - one_hot_encode_sequence(seq, max_len, alphabet)\n",
        "\n",
        "def load_trained_bundle(path: str, model_cls, device: str = \"cpu\"):\n",
        "    \"\"\"\n",
        "    Load a saved model and return (model, bundle dict).\n",
        "    \"\"\"\n",
        "    bundle = torch.load(path, map_location=device)\n",
        "    model = model_cls(**bundle[\"model_kwargs\"])\n",
        "    model.load_state_dict(bundle[\"state_dict\"])\n",
        "    model.eval()\n",
        "    return model.to(device), bundle\n",
        "\n",
        "def read_fasta(path: str):\n",
        "    \"\"\"\n",
        "    Returns a list of dicts: {\"id\":..., \"desc\":..., \"seq\":...} for each record.\n",
        "    \"\"\"\n",
        "    records = []\n",
        "    with open(path, \"r\") as f:\n",
        "        seq_id, desc, buf = None, \"\", []\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            if line.startswith(\">\"):\n",
        "                if seq_id is not None:\n",
        "                    records.append({\"id\": seq_id, \"desc\": desc, \"seq\": \"\".join(buf)})\n",
        "                header = line[1:]\n",
        "                parts = header.split(None, 1)\n",
        "                seq_id = parts[0]\n",
        "                desc = parts[1] if len(parts) > 1 else \"\"\n",
        "                buf = []\n",
        "            else:\n",
        "                buf.append(line)\n",
        "        if seq_id is not None:\n",
        "            records.append({\"id\": seq_id, \"desc\": desc, \"seq\": \"\".join(buf)})\n",
        "    return records\n",
        "\n",
        "def sanitize_seq(seq: str):\n",
        "    \"\"\"Remove whitespace and uppercase a sequence.\"\"\"\n",
        "    return re.sub(r\"\\s+\", \"\", (seq or \"\").upper())\n",
        "\n",
        "def predict_property(model, seq: str, *, max_len: int, alphabet: str):\n",
        "    \"\"\"\n",
        "    Encode a sequence and produce a scalar prediction.\n",
        "    \"\"\"\n",
        "    enc = one_hot_encode_sequence(seq, max_len, alphabet)  # (max_len, alphabet_size)\n",
        "    x = torch.tensor(enc, dtype=torch.float32).unsqueeze(0).to(next(model.parameters()).device)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        return float(model(x).squeeze().item())\n",
        "\n",
        "# ---------- LOAD ALL MODELS ----------\n",
        "# Each property gets its own model and encoding parameters.\n",
        "models_info = {}\n",
        "for prop in PROPERTY_NAMES:\n",
        "    path = os.path.join(MODEL_DIR, f\"fp_{prop}.pt\")\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Model file not found for {prop}: {path}\")\n",
        "    model, bundle = load_trained_bundle(path, PropertyPredictor, device=DEVICE)\n",
        "    models_info[prop] = {\n",
        "        \"model\": model,\n",
        "        \"max_len\": int(bundle[\"max_len\"]),\n",
        "        \"alphabet\": bundle[\"amino_alphabet\"],\n",
        "    }\n",
        "    print(f\"Loaded {prop} model with max_len={models_info[prop]['max_len']}\")\n",
        "\n",
        "# ---------- LOOP OVER ALL FASTA FILES ----------\n",
        "for FASTA_PATH in sorted(fasta_files):\n",
        "\n",
        "    # ---------- READ FASTA ----------\n",
        "    records = read_fasta(FASTA_PATH)\n",
        "    if not records:\n",
        "        print(f\"Skipping empty FASTA: {FASTA_PATH}\")\n",
        "        continue\n",
        "    print(\"\\nScoring:\", FASTA_PATH)\n",
        "    print(\"Number of sequences:\", len(records))\n",
        "\n",
        "    # ---------- PREDICT FOR ALL PROPERTIES ----------\n",
        "    rows = []\n",
        "    for rec in records:\n",
        "        seq_raw = sanitize_seq(rec[\"seq\"])\n",
        "        row = {\n",
        "            \"id\": rec[\"id\"],\n",
        "            \"desc\": rec[\"desc\"],\n",
        "            \"seq_len\": len(seq_raw),\n",
        "            \"sequence\": seq_raw,\n",
        "        }\n",
        "        # Track unknown/truncated characters per model (optional)\n",
        "        for prop in PROPERTY_NAMES:\n",
        "            info = models_info[prop]\n",
        "            alphabet_set = set(info[\"alphabet\"])\n",
        "            max_len = info[\"max_len\"]\n",
        "            row[f\"unknown_{prop}\"] = sum(1 for ch in seq_raw if ch not in alphabet_set)\n",
        "            row[f\"trunc_{prop}\"] = max(0, len(seq_raw) - max_len)\n",
        "            pred = predict_property(\n",
        "                info[\"model\"],\n",
        "                seq_raw,\n",
        "                max_len=max_len,\n",
        "                alphabet=info[\"alphabet\"],\n",
        "            )\n",
        "            row[f\"pred_{prop}\"] = pred\n",
        "        rows.append(row)\n",
        "\n",
        "    # ---------- BUILD DATAFRAME ----------\n",
        "    df = pd.DataFrame(rows)\n",
        "\n",
        "    # Example: sort by brightness descending\n",
        "    df_sorted = df.sort_values(by=\"pred_brightness\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "    # ---------- SAVE TO CSV ----------\n",
        "    fasta_base = os.path.splitext(os.path.basename(FASTA_PATH))[0]\n",
        "    out_csv = os.path.join(OUTPUT_PATH, fasta_base + \"_ANN_predictions_multi_parameters.csv\")\n",
        "    df_sorted.to_csv(out_csv, index=False)\n",
        "    print(\"Saved predictions:\", out_csv)\n"
      ],
      "metadata": {
        "id": "Vi2ttXqbmOlR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}